{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Tree-based models like Random Forest and XGBoost has become very poplular to address tabular(structured) data problems and gained a lot of tractions in Kaggle competitions. It has its very deserving reasons. A lot of the notebooks for this competition is inspired by fast.ai ML course. This notebook will also try to use fast.ai, but another approach: **Deep Learning**. \n",
    "This is a bit against industry consensous that Deep Learning is more for unstructured data like image, audio or NLP, and usually won't be very good at handling tabular data. Yet, the introduction of embedding for the categorical data changed this perspective and we'll try to use fast.ai's tabular model to tackle this competition and see how well a Deep Learning approach can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from fastai.data.transforms import Normalize\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "After imported the necessary fast.ai modules, mainly 'fastai.tabular'. Let's load the data in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset. Since the Test.csv and Valid.csv doesn't have label, it will be used to create our own validation set. \n",
    "train_df = pd.read_csv('train_split.csv', low_memory=False).drop(columns=['Id','BUTTER'])\n",
    "valid_df = pd.read_csv('validation.csv', low_memory=False).drop(columns=['Id','BUTTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_OWNPV_CHI2</th>\n",
       "      <th>B_IPCHI2_OWNPV</th>\n",
       "      <th>B_FDCHI2_OWNPV</th>\n",
       "      <th>B_DIRA_OWNPV</th>\n",
       "      <th>B_PT</th>\n",
       "      <th>Kst_892_0_IP_OWNPV</th>\n",
       "      <th>Kst_892_0_cosThetaH</th>\n",
       "      <th>Kplus_IP_OWNPV</th>\n",
       "      <th>Kplus_P</th>\n",
       "      <th>piminus_IP_OWNPV</th>\n",
       "      <th>piminus_P</th>\n",
       "      <th>gamma_PT</th>\n",
       "      <th>piminus_ETA</th>\n",
       "      <th>Kplus_ETA</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.878847</td>\n",
       "      <td>2.662533</td>\n",
       "      <td>2924.690991</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>19085.568945</td>\n",
       "      <td>0.569198</td>\n",
       "      <td>-0.575502</td>\n",
       "      <td>0.581565</td>\n",
       "      <td>66850.893711</td>\n",
       "      <td>0.637969</td>\n",
       "      <td>14298.486178</td>\n",
       "      <td>7940.694301</td>\n",
       "      <td>2.628526</td>\n",
       "      <td>2.680116</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.233566</td>\n",
       "      <td>0.092746</td>\n",
       "      <td>346.948714</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>6631.244546</td>\n",
       "      <td>0.248707</td>\n",
       "      <td>-0.615941</td>\n",
       "      <td>0.277898</td>\n",
       "      <td>39274.475071</td>\n",
       "      <td>0.148815</td>\n",
       "      <td>11553.163934</td>\n",
       "      <td>3904.681337</td>\n",
       "      <td>3.292504</td>\n",
       "      <td>3.085754</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.113632</td>\n",
       "      <td>2.442423</td>\n",
       "      <td>238.553023</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>7740.918989</td>\n",
       "      <td>0.222347</td>\n",
       "      <td>0.249383</td>\n",
       "      <td>0.216576</td>\n",
       "      <td>27757.153899</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>24081.196003</td>\n",
       "      <td>4738.891687</td>\n",
       "      <td>3.433676</td>\n",
       "      <td>3.121906</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.286133</td>\n",
       "      <td>6.337556</td>\n",
       "      <td>227.375132</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>6740.281614</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>0.591884</td>\n",
       "      <td>0.306927</td>\n",
       "      <td>10593.207077</td>\n",
       "      <td>0.400748</td>\n",
       "      <td>11343.521945</td>\n",
       "      <td>3308.943750</td>\n",
       "      <td>2.291867</td>\n",
       "      <td>2.200712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.474274</td>\n",
       "      <td>7.632751</td>\n",
       "      <td>106.730650</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>5556.388794</td>\n",
       "      <td>0.204273</td>\n",
       "      <td>0.655850</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>11801.249543</td>\n",
       "      <td>0.223101</td>\n",
       "      <td>25940.693317</td>\n",
       "      <td>4026.326871</td>\n",
       "      <td>3.290073</td>\n",
       "      <td>3.281829</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B_OWNPV_CHI2  B_IPCHI2_OWNPV  B_FDCHI2_OWNPV  B_DIRA_OWNPV          B_PT  \\\n",
       "0     28.878847        2.662533     2924.690991      0.999997  19085.568945   \n",
       "1     34.233566        0.092746      346.948714      0.999997   6631.244546   \n",
       "2     36.113632        2.442423      238.553023      0.999986   7740.918989   \n",
       "3     14.286133        6.337556      227.375132      0.999806   6740.281614   \n",
       "4     60.474274        7.632751      106.730650      0.999905   5556.388794   \n",
       "\n",
       "   Kst_892_0_IP_OWNPV  Kst_892_0_cosThetaH  Kplus_IP_OWNPV       Kplus_P  \\\n",
       "0            0.569198            -0.575502        0.581565  66850.893711   \n",
       "1            0.248707            -0.615941        0.277898  39274.475071   \n",
       "2            0.222347             0.249383        0.216576  27757.153899   \n",
       "3            0.347316             0.591884        0.306927  10593.207077   \n",
       "4            0.204273             0.655850        0.196600  11801.249543   \n",
       "\n",
       "   piminus_IP_OWNPV     piminus_P     gamma_PT  piminus_ETA  Kplus_ETA  signal  \n",
       "0          0.637969  14298.486178  7940.694301     2.628526   2.680116     1.0  \n",
       "1          0.148815  11553.163934  3904.681337     3.292504   3.085754     1.0  \n",
       "2          0.249840  24081.196003  4738.891687     3.433676   3.121906     1.0  \n",
       "3          0.400748  11343.521945  3308.943750     2.291867   2.200712     0.0  \n",
       "4          0.223101  25940.693317  4026.326871     3.290073   3.281829     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169935, 42727)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "The competition's evaluation methods uses RMSLE (root mean squared log error). So if we take the log of our prediction, we can just use the good old RMSE as our loss function. It's just easier this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining pre-processing we want for our fast.ai DataBunch\n",
    "procs=[Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namely, we'll fix the missing values, categorify all categorical columns, then normalize. Plain and simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{float64: ['B_OWNPV_CHI2', 'B_IPCHI2_OWNPV', 'B_FDCHI2_OWNPV', 'B_DIRA_OWNPV', 'B_PT', 'Kst_892_0_IP_OWNPV', 'Kst_892_0_cosThetaH', 'Kplus_IP_OWNPV', 'Kplus_P', 'piminus_IP_OWNPV', 'piminus_P', 'gamma_PT', 'piminus_ETA', 'Kplus_ETA', 'signal']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes\n",
    "g = train_df.columns.to_series().groupby(train_df.dtypes).groups\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at all the column types and see which are categorical and continuous. We'll use it to build the fast'ai DataBunch for training our learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['B_OWNPV_CHI2', 'B_IPCHI2_OWNPV', 'B_FDCHI2_OWNPV', 'B_DIRA_OWNPV',\n",
       "       'B_PT', 'Kst_892_0_IP_OWNPV', 'Kst_892_0_cosThetaH', 'Kplus_IP_OWNPV',\n",
       "       'Kplus_P', 'piminus_IP_OWNPV', 'piminus_P', 'gamma_PT', 'piminus_ETA',\n",
       "       'Kplus_ETA', 'signal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare categorical and continous data columns for building Tabular DataBunch.\n",
    "cat_vars = []\n",
    "\n",
    "cont_vars = ['B_OWNPV_CHI2', 'B_IPCHI2_OWNPV', 'B_FDCHI2_OWNPV',\n",
    "       'B_DIRA_OWNPV', 'B_PT', 'Kst_892_0_IP_OWNPV', 'Kst_892_0_cosThetaH',\n",
    "       'Kplus_IP_OWNPV', 'Kplus_P', 'piminus_IP_OWNPV', 'piminus_P',\n",
    "       'gamma_PT', 'piminus_ETA', 'Kplus_ETA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_OWNPV_CHI2</th>\n",
       "      <th>B_IPCHI2_OWNPV</th>\n",
       "      <th>B_FDCHI2_OWNPV</th>\n",
       "      <th>B_DIRA_OWNPV</th>\n",
       "      <th>B_PT</th>\n",
       "      <th>Kst_892_0_IP_OWNPV</th>\n",
       "      <th>Kst_892_0_cosThetaH</th>\n",
       "      <th>Kplus_IP_OWNPV</th>\n",
       "      <th>Kplus_P</th>\n",
       "      <th>piminus_IP_OWNPV</th>\n",
       "      <th>piminus_P</th>\n",
       "      <th>gamma_PT</th>\n",
       "      <th>piminus_ETA</th>\n",
       "      <th>Kplus_ETA</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.878847</td>\n",
       "      <td>2.662533</td>\n",
       "      <td>2924.690991</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>19085.568945</td>\n",
       "      <td>0.569198</td>\n",
       "      <td>-0.575502</td>\n",
       "      <td>0.581565</td>\n",
       "      <td>66850.893711</td>\n",
       "      <td>0.637969</td>\n",
       "      <td>14298.486178</td>\n",
       "      <td>7940.694301</td>\n",
       "      <td>2.628526</td>\n",
       "      <td>2.680116</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.233566</td>\n",
       "      <td>0.092746</td>\n",
       "      <td>346.948714</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>6631.244546</td>\n",
       "      <td>0.248707</td>\n",
       "      <td>-0.615941</td>\n",
       "      <td>0.277898</td>\n",
       "      <td>39274.475071</td>\n",
       "      <td>0.148815</td>\n",
       "      <td>11553.163934</td>\n",
       "      <td>3904.681337</td>\n",
       "      <td>3.292504</td>\n",
       "      <td>3.085754</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.113632</td>\n",
       "      <td>2.442423</td>\n",
       "      <td>238.553023</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>7740.918989</td>\n",
       "      <td>0.222347</td>\n",
       "      <td>0.249383</td>\n",
       "      <td>0.216576</td>\n",
       "      <td>27757.153899</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>24081.196003</td>\n",
       "      <td>4738.891687</td>\n",
       "      <td>3.433676</td>\n",
       "      <td>3.121906</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.286133</td>\n",
       "      <td>6.337556</td>\n",
       "      <td>227.375132</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>6740.281614</td>\n",
       "      <td>0.347316</td>\n",
       "      <td>0.591884</td>\n",
       "      <td>0.306927</td>\n",
       "      <td>10593.207077</td>\n",
       "      <td>0.400748</td>\n",
       "      <td>11343.521945</td>\n",
       "      <td>3308.943750</td>\n",
       "      <td>2.291867</td>\n",
       "      <td>2.200712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.474274</td>\n",
       "      <td>7.632751</td>\n",
       "      <td>106.730650</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>5556.388794</td>\n",
       "      <td>0.204273</td>\n",
       "      <td>0.655850</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>11801.249543</td>\n",
       "      <td>0.223101</td>\n",
       "      <td>25940.693317</td>\n",
       "      <td>4026.326871</td>\n",
       "      <td>3.290073</td>\n",
       "      <td>3.281829</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169930</th>\n",
       "      <td>12.893907</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>984.507893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11121.462006</td>\n",
       "      <td>0.491973</td>\n",
       "      <td>-0.548488</td>\n",
       "      <td>0.541038</td>\n",
       "      <td>72639.491690</td>\n",
       "      <td>0.398007</td>\n",
       "      <td>22999.329625</td>\n",
       "      <td>3426.012947</td>\n",
       "      <td>3.220792</td>\n",
       "      <td>3.165776</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169931</th>\n",
       "      <td>33.269855</td>\n",
       "      <td>0.422886</td>\n",
       "      <td>528.518223</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>6547.421388</td>\n",
       "      <td>0.324063</td>\n",
       "      <td>-0.397384</td>\n",
       "      <td>0.286336</td>\n",
       "      <td>50331.007785</td>\n",
       "      <td>0.539486</td>\n",
       "      <td>8898.424053</td>\n",
       "      <td>3657.986285</td>\n",
       "      <td>3.077444</td>\n",
       "      <td>3.243626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169932</th>\n",
       "      <td>8.028983</td>\n",
       "      <td>2.256855</td>\n",
       "      <td>377.706643</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>5160.700365</td>\n",
       "      <td>0.626228</td>\n",
       "      <td>-0.176330</td>\n",
       "      <td>0.598550</td>\n",
       "      <td>16262.726940</td>\n",
       "      <td>0.705622</td>\n",
       "      <td>5497.144994</td>\n",
       "      <td>4276.250211</td>\n",
       "      <td>2.379503</td>\n",
       "      <td>2.772514</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169933</th>\n",
       "      <td>59.286201</td>\n",
       "      <td>3.385165</td>\n",
       "      <td>675.095332</td>\n",
       "      <td>0.999828</td>\n",
       "      <td>9506.037618</td>\n",
       "      <td>0.278407</td>\n",
       "      <td>0.948246</td>\n",
       "      <td>0.231088</td>\n",
       "      <td>11309.374549</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>20879.660445</td>\n",
       "      <td>3915.660229</td>\n",
       "      <td>2.323603</td>\n",
       "      <td>2.348653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169934</th>\n",
       "      <td>22.737875</td>\n",
       "      <td>7.698472</td>\n",
       "      <td>286.450638</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>14869.693203</td>\n",
       "      <td>0.247850</td>\n",
       "      <td>-0.159846</td>\n",
       "      <td>0.247443</td>\n",
       "      <td>85036.225493</td>\n",
       "      <td>0.287413</td>\n",
       "      <td>50546.761176</td>\n",
       "      <td>8233.165297</td>\n",
       "      <td>3.680494</td>\n",
       "      <td>3.676613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169935 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        B_OWNPV_CHI2  B_IPCHI2_OWNPV  B_FDCHI2_OWNPV  B_DIRA_OWNPV  \\\n",
       "0          28.878847        2.662533     2924.690991      0.999997   \n",
       "1          34.233566        0.092746      346.948714      0.999997   \n",
       "2          36.113632        2.442423      238.553023      0.999986   \n",
       "3          14.286133        6.337556      227.375132      0.999806   \n",
       "4          60.474274        7.632751      106.730650      0.999905   \n",
       "...              ...             ...             ...           ...   \n",
       "169930     12.893907        0.151000      984.507893      1.000000   \n",
       "169931     33.269855        0.422886      528.518223      0.999999   \n",
       "169932      8.028983        2.256855      377.706643      0.999878   \n",
       "169933     59.286201        3.385165      675.095332      0.999828   \n",
       "169934     22.737875        7.698472      286.450638      0.999711   \n",
       "\n",
       "                B_PT  Kst_892_0_IP_OWNPV  Kst_892_0_cosThetaH  Kplus_IP_OWNPV  \\\n",
       "0       19085.568945            0.569198            -0.575502        0.581565   \n",
       "1        6631.244546            0.248707            -0.615941        0.277898   \n",
       "2        7740.918989            0.222347             0.249383        0.216576   \n",
       "3        6740.281614            0.347316             0.591884        0.306927   \n",
       "4        5556.388794            0.204273             0.655850        0.196600   \n",
       "...              ...                 ...                  ...             ...   \n",
       "169930  11121.462006            0.491973            -0.548488        0.541038   \n",
       "169931   6547.421388            0.324063            -0.397384        0.286336   \n",
       "169932   5160.700365            0.626228            -0.176330        0.598550   \n",
       "169933   9506.037618            0.278407             0.948246        0.231088   \n",
       "169934  14869.693203            0.247850            -0.159846        0.247443   \n",
       "\n",
       "             Kplus_P  piminus_IP_OWNPV     piminus_P     gamma_PT  \\\n",
       "0       66850.893711          0.637969  14298.486178  7940.694301   \n",
       "1       39274.475071          0.148815  11553.163934  3904.681337   \n",
       "2       27757.153899          0.249840  24081.196003  4738.891687   \n",
       "3       10593.207077          0.400748  11343.521945  3308.943750   \n",
       "4       11801.249543          0.223101  25940.693317  4026.326871   \n",
       "...              ...               ...           ...          ...   \n",
       "169930  72639.491690          0.398007  22999.329625  3426.012947   \n",
       "169931  50331.007785          0.539486   8898.424053  3657.986285   \n",
       "169932  16262.726940          0.705622   5497.144994  4276.250211   \n",
       "169933  11309.374549          0.308349  20879.660445  3915.660229   \n",
       "169934  85036.225493          0.287413  50546.761176  8233.165297   \n",
       "\n",
       "        piminus_ETA  Kplus_ETA  signal  \n",
       "0          2.628526   2.680116     1.0  \n",
       "1          3.292504   3.085754     1.0  \n",
       "2          3.433676   3.121906     1.0  \n",
       "3          2.291867   2.200712     0.0  \n",
       "4          3.290073   3.281829     0.0  \n",
       "...             ...        ...     ...  \n",
       "169930     3.220792   3.165776     0.0  \n",
       "169931     3.077444   3.243626     1.0  \n",
       "169932     2.379503   2.772514     1.0  \n",
       "169933     2.323603   2.348653     0.0  \n",
       "169934     3.680494   3.676613     0.0  \n",
       "\n",
       "[169935 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange training set before feed into the databunch\n",
    "dep_var = 'signal'\n",
    "df = train_df[cat_vars + cont_vars + [dep_var]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create our validation set. The most important step. Since this dataset is somewhat time series, we need to make sure validation set entries happens AFTER all entries in the training set, otherwise the model will be cheating and won't generalize well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.data import TabularDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fast.ai datablock api to put our training data into the DataBunch, getting ready for training\n",
    "data = TabularDataLoaders.from_df(df,  cat_names=cat_vars, cont_names=cont_vars, procs=procs,y_names='signal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_OWNPV_CHI2</th>\n",
       "      <th>B_IPCHI2_OWNPV</th>\n",
       "      <th>B_FDCHI2_OWNPV</th>\n",
       "      <th>B_DIRA_OWNPV</th>\n",
       "      <th>B_PT</th>\n",
       "      <th>Kst_892_0_IP_OWNPV</th>\n",
       "      <th>Kst_892_0_cosThetaH</th>\n",
       "      <th>Kplus_IP_OWNPV</th>\n",
       "      <th>Kplus_P</th>\n",
       "      <th>piminus_IP_OWNPV</th>\n",
       "      <th>piminus_P</th>\n",
       "      <th>gamma_PT</th>\n",
       "      <th>piminus_ETA</th>\n",
       "      <th>Kplus_ETA</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.068467</td>\n",
       "      <td>1.471852</td>\n",
       "      <td>1668.456170</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>12160.437408</td>\n",
       "      <td>0.546918</td>\n",
       "      <td>-0.199049</td>\n",
       "      <td>0.643068</td>\n",
       "      <td>31573.789036</td>\n",
       "      <td>0.400070</td>\n",
       "      <td>21161.294931</td>\n",
       "      <td>7545.394002</td>\n",
       "      <td>2.887407</td>\n",
       "      <td>2.923944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.170727</td>\n",
       "      <td>2.205191</td>\n",
       "      <td>237.649367</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>4611.243265</td>\n",
       "      <td>0.269019</td>\n",
       "      <td>-0.204553</td>\n",
       "      <td>0.334923</td>\n",
       "      <td>10187.023790</td>\n",
       "      <td>0.184128</td>\n",
       "      <td>8503.592415</td>\n",
       "      <td>3647.614220</td>\n",
       "      <td>2.782956</td>\n",
       "      <td>2.380438</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.954386</td>\n",
       "      <td>4.815278</td>\n",
       "      <td>318.533618</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>8162.881784</td>\n",
       "      <td>0.239195</td>\n",
       "      <td>-0.620043</td>\n",
       "      <td>0.259206</td>\n",
       "      <td>67188.006805</td>\n",
       "      <td>0.226728</td>\n",
       "      <td>16899.859150</td>\n",
       "      <td>3513.940735</td>\n",
       "      <td>3.598323</td>\n",
       "      <td>3.394847</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.126931</td>\n",
       "      <td>1.010613</td>\n",
       "      <td>141.363170</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>10409.026333</td>\n",
       "      <td>0.207534</td>\n",
       "      <td>0.845583</td>\n",
       "      <td>0.183372</td>\n",
       "      <td>9423.700688</td>\n",
       "      <td>0.235512</td>\n",
       "      <td>20424.845633</td>\n",
       "      <td>8933.822210</td>\n",
       "      <td>3.269624</td>\n",
       "      <td>3.455156</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.543175</td>\n",
       "      <td>6.694719</td>\n",
       "      <td>1358.092348</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>5369.266572</td>\n",
       "      <td>0.368295</td>\n",
       "      <td>-0.229230</td>\n",
       "      <td>0.463762</td>\n",
       "      <td>10638.671141</td>\n",
       "      <td>0.252619</td>\n",
       "      <td>8251.985794</td>\n",
       "      <td>3681.267843</td>\n",
       "      <td>2.847020</td>\n",
       "      <td>2.538678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.046338</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>575.715647</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>10972.552768</td>\n",
       "      <td>0.381012</td>\n",
       "      <td>0.083197</td>\n",
       "      <td>0.445311</td>\n",
       "      <td>18344.432063</td>\n",
       "      <td>0.338042</td>\n",
       "      <td>18515.908330</td>\n",
       "      <td>5858.030274</td>\n",
       "      <td>2.600619</td>\n",
       "      <td>2.381447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.545283</td>\n",
       "      <td>3.161073</td>\n",
       "      <td>5675.258246</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>9714.529301</td>\n",
       "      <td>2.128631</td>\n",
       "      <td>-0.364013</td>\n",
       "      <td>2.092698</td>\n",
       "      <td>18613.690025</td>\n",
       "      <td>0.797955</td>\n",
       "      <td>9480.605847</td>\n",
       "      <td>6362.613294</td>\n",
       "      <td>2.500552</td>\n",
       "      <td>2.531930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62.182065</td>\n",
       "      <td>4.493889</td>\n",
       "      <td>297.555960</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>7516.475617</td>\n",
       "      <td>0.205090</td>\n",
       "      <td>-0.219213</td>\n",
       "      <td>0.277780</td>\n",
       "      <td>6659.065614</td>\n",
       "      <td>0.139449</td>\n",
       "      <td>7538.023205</td>\n",
       "      <td>5542.593749</td>\n",
       "      <td>2.482442</td>\n",
       "      <td>2.369295</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.599662</td>\n",
       "      <td>3.355160</td>\n",
       "      <td>656.072323</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>13040.566425</td>\n",
       "      <td>0.329322</td>\n",
       "      <td>-0.315135</td>\n",
       "      <td>0.283032</td>\n",
       "      <td>66975.312773</td>\n",
       "      <td>0.502113</td>\n",
       "      <td>18291.413885</td>\n",
       "      <td>3647.159631</td>\n",
       "      <td>2.851285</td>\n",
       "      <td>2.821613</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.712267</td>\n",
       "      <td>5.727160</td>\n",
       "      <td>3668.974616</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>5840.807050</td>\n",
       "      <td>0.881368</td>\n",
       "      <td>0.137922</td>\n",
       "      <td>0.895917</td>\n",
       "      <td>20967.991946</td>\n",
       "      <td>0.934745</td>\n",
       "      <td>13767.696990</td>\n",
       "      <td>3793.189221</td>\n",
       "      <td>3.027520</td>\n",
       "      <td>3.145737</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time for some training. We will fire up a fast.ai 'tabular.learner' from the DataBunch we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.learner import tabular_learner\n",
    "from fastai.tabular.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, '.', procs=procs, cat_names=cat_vars, cont_names=cont_vars, \n",
    "                                 y_names=\"signal\", bs=64)\n",
    "learn = tabular_learner(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList()\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=14, out_features=200, bias=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): LinBnDrop(\n",
       "      (0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=200, out_features=100, bias=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): LinBnDrop(\n",
       "      (0): Linear(in_features=100, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, we have embedding layers for categorical columns, then followed by a drop out layer. We have batch norm layer for the continuous columns, then we put all of them into two fully connected layers with 1000 and 500 nodes, with Relu, BatchNorm, and Dropout in between. Quite standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row, clas, probs = learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1176]), tensor([0.1176]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clas, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User fast.ai's *lr_find* function to find the proper learning rate, then do a 'fit one cycle' training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'TabularModel' object has no attribute 'fit_one_cycle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-47c19a11e9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'TabularModel' object has no attribute 'fit_one_cycle'"
     ]
    }
   ],
   "source": [
    "learn.model.fit_one_cycle(2, 1e-2, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TabularLearner' object has no attribute 'fit_one_cycle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-378e5906660e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TabularLearner' object has no attribute 'fit_one_cycle'"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 3e-4, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(5, 3e-4, wd=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result reaches 0.227 RMSLE, I think it beats the #1 in Kaggle leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion\n",
    "I think overall people still prefer XGBoost or Random Forest for tabular Kaggle competitions since it usually will yield the best scores. However, Deep Learning is also a viable approach, though lacking a bit on the explainability side. At least it could be used for ensamble purpose so it's worth exploring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingFeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Feature generator from a gradient boosting\n",
    "\n",
    "    References:\n",
    "        - Practical Lessons from Predicting Clicks on Ads at Facebook\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stack_to_X=True,\n",
    "        sparse_feat=True,\n",
    "        add_probs=True,\n",
    "        criterion=\"friedman_mse\",\n",
    "        init=None,\n",
    "        learning_rate=0.1,\n",
    "        loss=\"deviance\",\n",
    "        max_depth=3,\n",
    "        max_features=None,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_impurity_split=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        n_estimators=50,\n",
    "        n_iter_no_change=None,\n",
    "        presort=\"auto\",\n",
    "        random_state=None,\n",
    "        subsample=1.0,\n",
    "        tol=0.0001,\n",
    "        validation_fraction=0.1,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "    ):\n",
    "\n",
    "        # Deciding wheather to append features or simply return generated features\n",
    "        self.stack_to_X = stack_to_X\n",
    "        self.sparse_feat = sparse_feat\n",
    "        self.add_probs = add_probs\n",
    "\n",
    "        # GBM hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.init = init\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.presort = presort\n",
    "        self.random_state = random_state\n",
    "        self.subsample = subsample\n",
    "        self.tol = tol\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "\n",
    "    def _get_leaves(self, X):\n",
    "        X_leaves = self.gbm.apply(X)\n",
    "        n_rows, n_cols, _ = X_leaves.shape\n",
    "        X_leaves = X_leaves.reshape(n_rows, n_cols)\n",
    "\n",
    "        return X_leaves\n",
    "\n",
    "    def _decode_leaves(self, X):\n",
    "\n",
    "        if self.sparse_feat:\n",
    "            # float_eltype = np.float32\n",
    "            # return scipy.sparse.csr.csr_matrix(self.encoder.transform(X), dtype=float_eltype)\n",
    "            return scipy.sparse.csr.csr_matrix(self.encoder.transform(X))\n",
    "        else:\n",
    "            return self.encoder.transform(X).todense()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.gbm = GradientBoostingClassifier(\n",
    "            criterion=self.criterion,\n",
    "            init=self.init,\n",
    "            learning_rate=self.learning_rate,\n",
    "            loss=self.loss,\n",
    "            max_depth=self.max_depth,\n",
    "            max_features=self.max_features,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            min_impurity_split=self.min_impurity_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "            n_estimators=self.n_estimators,\n",
    "            n_iter_no_change=self.n_iter_no_change,\n",
    "            presort=self.presort,\n",
    "            random_state=self.random_state,\n",
    "            subsample=self.subsample,\n",
    "            tol=self.tol,\n",
    "            validation_fraction=self.validation_fraction,\n",
    "            verbose=self.verbose,\n",
    "            warm_start=self.warm_start,\n",
    "        )\n",
    "\n",
    "        self.gbm.fit(X, y)\n",
    "        self.encoder = OneHotEncoder(categories=\"auto\")\n",
    "        X_leaves = self._get_leaves(X)\n",
    "        self.encoder.fit(X_leaves)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Generates leaves features using the fitted self.gbm and saves them in R.\n",
    "        If 'self.stack_to_X==True' then '.transform' returns the original features with 'R' appended as columns.\n",
    "        If 'self.stack_to_X==False' then  '.transform' returns only the leaves features from 'R'\n",
    "        Ìf 'self.sparse_feat==True' then the input matrix from 'X' is cast as a sparse matrix as well as the 'R' matrix.\n",
    "        \"\"\"\n",
    "        R = self._decode_leaves(self._get_leaves(X))\n",
    "\n",
    "        if self.sparse_feat:\n",
    "            if self.add_probs:\n",
    "                P = self.gbm.predict_proba(X)\n",
    "                X_new = (\n",
    "                    scipy.sparse.hstack(\n",
    "                        (\n",
    "                            scipy.sparse.csr.csr_matrix(X),\n",
    "                            R,\n",
    "                            scipy.sparse.csr.csr_matrix(P),\n",
    "                        )\n",
    "                    )\n",
    "                    if self.stack_to_X == True\n",
    "                    else R\n",
    "                )\n",
    "            else:\n",
    "                X_new = (\n",
    "                    scipy.sparse.hstack((scipy.sparse.csr.csr_matrix(X), R))\n",
    "                    if self.stack_to_X == True\n",
    "                    else R\n",
    "                )\n",
    "\n",
    "        else:\n",
    "\n",
    "            if self.add_probs:\n",
    "                P = self.gbm.predict_proba(X)\n",
    "                X_new = (\n",
    "                    scipy.sparse.hstack(\n",
    "                        (\n",
    "                            scipy.sparse.csr.csr_matrix(X),\n",
    "                            R,\n",
    "                            scipy.sparse.csr.csr_matrix(P),\n",
    "                        )\n",
    "                    )\n",
    "                    if self.stack_to_X == True\n",
    "                    else R\n",
    "                )\n",
    "            else:\n",
    "                X_new = np.hstack((X, R)) if self.stack_to_X == True else R\n",
    "\n",
    "        return X_new.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-556f31746a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "#from sktools import GradientBoostingFeatureGenerator\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()['data']\n",
    "y = load_boston()['target']\n",
    "y = np.where(y>y.mean(),1,0)\n",
    "mf = GradientBoostingFeatureGenerator(sparse_feat=False)\n",
    "mf.fit(boston, y)\n",
    "mf.transform(boston.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
